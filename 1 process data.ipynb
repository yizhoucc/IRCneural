{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data processing\n",
    "refactored to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('privateconfig'))\n",
    "resdir = Path(config['Datafolder']['data'])\n",
    "workdir = Path(config['Codefolder']['workspace'])\n",
    "os.chdir(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task\n",
    "from env_config import Config\n",
    "from firefly_task import ffacc_real\n",
    "# from monkey_functions import *\n",
    "# from InverseFuncs import *\n",
    "from stable_baselines3 import TD3\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_plot_ult import *\n",
    "import time\n",
    "tic=time.time()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre IRC\n",
    "\n",
    "convert the mat data file (with neural data) into (states, actions, tasks) for IRC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const\n",
    "bin_size = 17  # how many bin of DT. about 0.1 s\n",
    "# num_bins = 24  # how many bins to use. use 2.4 s and discard the long trials.\n",
    "# monkey_height = 10\n",
    "DT = 0.006  # DT for raw data\n",
    "# reward_boundary = 65\n",
    "areas = ['PPC', 'PFC', 'MST']\n",
    "worldscale = 200\n",
    "\n",
    "\n",
    "m = 'm51'\n",
    "folder = 'm51_mat_ruiyi'\n",
    "dens = [0.0001, 0.0005, 0.001,  0.005]\n",
    "\n",
    "locals().update({m: {}})\n",
    "figure_path = resdir/'figures'\n",
    "# datapaths = [i for i in Pa1th(resdir/'mat_ruiyi').glob(f'{m}*.mat')]\n",
    "datapaths = [i for i in Path(resdir/folder).glob(f'{m}*.mat')]\n",
    "session=[int(a.stem[-2:]) for a in datapaths]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from raw data file: task relavent variables and neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data\n",
    "df=pd.DataFrame() \n",
    "for idx, datapath in enumerate(datapaths):\n",
    "    if datapath.stem[-1].isalpha():\n",
    "        continue\n",
    "    data = loadmat(datapath)\n",
    "    eval(m)[datapath.stem] = data\n",
    "\n",
    "    # df\n",
    "    sessdf=pd.DataFrame() \n",
    "    sessdf['trial']=np.arange((len(data['trials_behv'][0])))\n",
    "    sessdf['session']=int(datapath.stem[(datapath.stem).find('s')+1:])\n",
    "    df=pd.concat([df, sessdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessdata = defaultdict(list)\n",
    "\n",
    "for key, data in eval(m).items():\n",
    "    sess = int(key[-2:])\n",
    "    if key[-1].isalpha():\n",
    "        continue\n",
    "\n",
    "    trials_behv = data['trials_behv'][0]\n",
    "    trials_units = data['units'][0]\n",
    "    units_area = np.array([v[0] for v in trials_units['brain_area']])\n",
    "    trials_error = []\n",
    "    trials_error_sign = []\n",
    "    trials_target_angle = []\n",
    "    trials_target_distance = []\n",
    "\n",
    "    for trial_idx, trial_behv in enumerate(trials_behv):\n",
    "        trial_ts = trial_behv['continuous']['ts'][0][0].reshape(-1)\n",
    "        t_mask = (trial_ts > 0) & (\n",
    "            ~np.isnan(trial_behv['continuous']['ymp'][0][0].reshape(-1)))\n",
    "        t_mask &= trial_ts < trial_behv['events']['t_stop'][0][0].reshape(-1)\n",
    "        if t_mask.sum() > 0:\n",
    "            # remove the first data point to avoid downsample error\n",
    "            t_mask[np.where(t_mask == True)[0][0]] = False\n",
    "\n",
    "        # task varaibles from data\n",
    "        mx = trial_behv['continuous']['xmp'][0][0][t_mask]\n",
    "        my = trial_behv['continuous']['ymp'][0][0][t_mask]\n",
    "        fx = trial_behv['continuous']['xfp'][0][0][t_mask]\n",
    "        fy = trial_behv['continuous']['yfp'][0][0][t_mask]\n",
    "        eye_hor_theta = trial_behv['continuous']['yre'][0][0][t_mask] # use yle for left eye.\n",
    "        eye_ver_theta = trial_behv['continuous']['zre'][0][0][t_mask]\n",
    "        # print(len(eye_hor_theta), len(eye_ver_theta), len(mx))\n",
    "        mv = trial_behv['continuous']['v'][0][0][t_mask].reshape(-1, 1)\n",
    "        mw = trial_behv['continuous']['w'][0][0][t_mask].reshape(-1, 1)\n",
    "\n",
    "        # some adjustment for screen distance\n",
    "        sx = np.ones_like(fx)\n",
    "        sy = np.ones_like(fy)\n",
    "        if my.size > 0:\n",
    "            fx = np.ones_like(fx) * fx[0]\n",
    "            fy = np.ones_like(fy) * fy[0]\n",
    "            sx *= mx[-1]\n",
    "            sy *= my[-1]\n",
    "            my = my + 30\n",
    "            fy = fy + 30\n",
    "            sy = sy + 30\n",
    "\n",
    "        # some thing coudl be removed from there.\n",
    "        dx = fx - mx; dy = fy - my\n",
    "        rel_dist = np.sqrt(dx**2 + dy**2); rel_ang = np.rad2deg(np.arctan2(dy, dx))\n",
    "        rel_dist_stop = np.sqrt((sx - mx)**2 + (sy - my)**2)\n",
    "        abs_dist = np.sqrt(mx**2 + my**2); abs_ang = np.rad2deg(np.arctan2(my, mx))\n",
    "        body_theta = np.deg2rad(np.cumsum(mw*-1) * DT + 90)\n",
    "        body_x, body_y = mx.reshape(-1), my.reshape(-1)\n",
    "\n",
    "\n",
    "        # skip bad trial\n",
    "        if t_mask.sum() * DT > 3.5 or t_mask.sum() * DT < 0.6 or mv.max() < 50 or \\\n",
    "                abs_dist[-1] < np.sqrt(fx**2 + fy**2)[-1] * 0.3:\n",
    "            continue\n",
    "\n",
    "        # errors\n",
    "        if my.size > 0:\n",
    "            trials_error.append(rel_dist[-1][0])\n",
    "            trials_error_sign.append(rel_dist[-1][0])\n",
    "            trials_target_angle.append(\n",
    "                np.rad2deg(np.arctan2(fy, fx))[-1][0] - 90)\n",
    "            trials_target_distance.append(np.sqrt(fx**2 + fy**2)[-1][0])\n",
    "            d1 = np.sqrt(fx**2 + fy**2)\n",
    "            r1 = (fx**2 + fy**2) / (2*fx)\n",
    "            radian1 = 2 * r1 * np.arcsin(d1 / (2 * r1))\n",
    "            d2 = np.sqrt(mx**2 + my**2)\n",
    "            r2 = (mx**2 + my**2) / (2*mx + 1e-8)\n",
    "            radian2 = 2 * r2 * np.arcsin(d2 / (2 * r2 + 1e-8))\n",
    "            sign = np.ones_like(rel_dist)\n",
    "            sign[radian2 < radian1] = -1\n",
    "            rel_dist = sign * rel_dist\n",
    "            trials_error_sign[-1] = rel_dist[-1][0]\n",
    "        else:\n",
    "            trials_error.append(np.nan)\n",
    "            trials_error_sign.append(np.nan)\n",
    "            trials_target_angle.append(np.nan)\n",
    "            trials_target_distance.append(np.nan)\n",
    "\n",
    "        target_variable = np.hstack([rel_dist, rel_ang, abs_dist, abs_ang,\n",
    "                                     eye_hor_theta, eye_ver_theta, \n",
    "                                     fx, fy, mx, my, mv, mw])\n",
    "\n",
    "        target_variable_ds = downsample(target_variable, bin_size=bin_size)\n",
    "\n",
    "        (rel_dist, rel_ang, abs_dist, abs_ang,\n",
    "         eye_hor_theta, eye_ver_theta,\n",
    "         fx, fy, mx, my, mv, mw) = zip(*target_variable_ds)\n",
    "        body_theta = -np.deg2rad(np.cumsum(mw) * 0.1 - 90)\n",
    "        body_x, body_y = np.array(mx).reshape(-1), np.array(my).reshape(-1)\n",
    "        latent_ff_hori, latent_ff_vert = convert_location_to_angle(abs(np.array(rel_dist)).reshape(-1), np.array(fx).reshape(-1), np.array(fy).reshape(-1),\n",
    "                                                                   np.array(body_theta), np.array(\n",
    "                                                                       body_x), np.array(body_y),\n",
    "                                                                   np.array(eye_hor_theta).reshape(-1), np.array(eye_ver_theta).reshape(-1), remove_pre=False)\n",
    "\n",
    "        # df\n",
    "        sessdata['session'].append(int(sess))\n",
    "        sessdata['trial'].append(trial_idx)\n",
    "        sessdata['fullon'].append(trial_behv['logical']['firefly_fullON'][0][0][0][0])\n",
    "        sessdata['density'].append(\n",
    "            (trial_behv['prs'][0][0]['floordensity'].item()))\n",
    "        # sessdata['rel_dist'].append(rel_dist)\n",
    "        # sessdata['rel_ang'].append(rel_ang)\n",
    "        # sessdata['abs_dist'].append(abs_dist)\n",
    "        # sessdata['abs_ang'].append(abs_ang)\n",
    "        sessdata['eye_hori'].append(eye_hor_theta)\n",
    "        sessdata['eye_vert'].append(eye_ver_theta)\n",
    "        sessdata['ff_hori'].append(latent_ff_hori.reshape(-1))\n",
    "        sessdata['ff_vert'].append(latent_ff_vert.reshape(-1))\n",
    "        sessdata['fx'].append(fx)\n",
    "        sessdata['fy'].append(fy)\n",
    "        sessdata['mx'].append(mx)\n",
    "        sessdata['my'].append(my)\n",
    "        sessdata['mv'].append(mv)\n",
    "        sessdata['mw'].append(mw)\n",
    "\n",
    "        # neural\n",
    "        activities = []  # activities for all neurons for 1 trial. shape: ts, neurons\n",
    "        for trials_unit in trials_units:\n",
    "            fire_ts = trials_unit['trials'][0][trial_idx][0].reshape(-1)\n",
    "            if fire_ts.size > 0 and fire_ts[-1] >= trial_ts[-1]:\n",
    "                fire_ts = fire_ts[:-1]\n",
    "            activity = np.zeros_like(trial_ts)\n",
    "            bin_indices = np.digitize(fire_ts, trial_ts)\n",
    "            unique_bins, bin_counts = np.unique(\n",
    "                bin_indices, return_counts=True)\n",
    "            activity[unique_bins] = bin_counts\n",
    "            activities.append(activity)\n",
    "\n",
    "        activities = np.vstack(activities).T   # time * unit\n",
    "        activities = activities[t_mask]\n",
    "        activities = gaussian_filter1d(\n",
    "            activities, sigma=4, axis=0)  # neural for each trial\n",
    "        activityds = downsample(activities, bin_size=bin_size)\n",
    "\n",
    "        for area in areas:\n",
    "            area_mask = [v in area for v in units_area]\n",
    "            if sum(area_mask) == 0:\n",
    "                activity_ = np.nan\n",
    "            else:\n",
    "                activity_ = activityds[:, area_mask]  # area activity\n",
    "            sessdata[area].append(activity_)\n",
    "\n",
    "    sessdata['error'] += (trials_error)\n",
    "    sessdata['error_sign'] += (trials_error_sign)\n",
    "    sessdata['target_angle'] += (trials_target_angle)\n",
    "    sessdata['target_distance'] += (trials_target_distance)\n",
    "\n",
    "tmp = pd.DataFrame(sessdata)\n",
    "df = pd.merge(df, tmp, on=['trial', 'session'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRC input data (state, action, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessdata=defaultdict(list)\n",
    "\n",
    "for sess in session:\n",
    "    states, actions, tasks=[],[],[]\n",
    "    sessdf=df[df.session==sess]\n",
    "    trial_idces=sessdf.trial\n",
    "\n",
    "    for trial_idx in trial_idces:\n",
    "        trialdf=sessdf[sessdf.trial==trial_idx]\n",
    "        trialdata=trialdf.iloc[0]\n",
    "        # task\n",
    "        taskx = (trialdata.fx[0] - trialdata.mx[0]).astype('float32'); tasky = (trialdata.fy[0] - trialdata.my[0]).astype('float32')\n",
    "        tasks.append([tasky/worldscale,taskx/worldscale])\n",
    "        # actions\n",
    "        trialaction=np.stack([trialdata.mv,trialdata.mw]).T\n",
    "        trialaction[:,0]=trialaction[:,0]/worldscale # v need reduce scale\n",
    "        trialaction[:,1]=trialaction[:,1]/180*pi\n",
    "        actions.append(trialaction.astype('float32'))\n",
    "\n",
    "        # states from run the actions\n",
    "        px, py, heading, v, w = 0,0,0,0,0\n",
    "        log=[]\n",
    "        for a in trialaction:\n",
    "            px, py, heading, v, w=state_step2(px, py, heading, v, w, a, dt=0.1,userad=True)\n",
    "            log.append([px, py, heading, v, w])\n",
    "        px, py, heading, v, w=state_step2(px, py, heading, v, w, a, dt=0.1,userad=True)\n",
    "        log.append([px, py, heading, v, w])\n",
    "        trialstates=np.array(log)[1:]\n",
    "        \n",
    "        states.append(trialstates.astype('float32'))\n",
    "\n",
    "        sessdata['session'].append(sess)\n",
    "        sessdata['trial'].append(trial_idx)\n",
    "        sessdata['state'].append(trialstates.astype('float32'))  \n",
    "        sessdata['action'].append(trialaction.astype('float32'))  \n",
    "        sessdata['task'].append([tasky/worldscale,taskx/worldscale])  \n",
    "\n",
    "tmp=pd.DataFrame(sessdata)\n",
    "df = pd.merge(df, tmp, on=['trial','session'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute belief "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model estimated likelihood (negative log likelihood)\n",
    "torch.manual_seed(42)\n",
    "arg = Config()\n",
    "\n",
    "env = ffacc_real.FireFlyPaper(arg)\n",
    "env.debug=True\n",
    "phi = torch.tensor([[0.5],\n",
    "                    [pi/2],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    [0.13],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    ])\n",
    "\n",
    "agent_ = TD3.load(workdir/'trained_agent/paper')\n",
    "agent = agent_.actor.mu.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preirc_den_0\n",
      "/Users/yc/Documents/lab_data/m51_mat_ruiyi/m51_0preirc_den_0\n",
      "using ind:  -1 final logll :  16.993248803274973\n",
      "tensor([[0.9924],\n",
      "        [0.7905],\n",
      "        [0.7104],\n",
      "        [0.1499]])\n",
      "preirc_den_1\n",
      "/Users/yc/Documents/lab_data/m51_mat_ruiyi/m51_1preirc_den_1\n",
      "using ind:  -1 final logll :  15.665690626416888\n",
      "tensor([[1.1921],\n",
      "        [0.8381],\n",
      "        [0.6041],\n",
      "        [0.1469]])\n",
      "preirc_den_2\n",
      "/Users/yc/Documents/lab_data/m51_mat_ruiyi/m51_2preirc_den_2\n",
      "using ind:  -1 final logll :  16.006959642682755\n",
      "tensor([[1.1985],\n",
      "        [0.9138],\n",
      "        [0.5796],\n",
      "        [0.1894]])\n",
      "preirc_den_3\n",
      "/Users/yc/Documents/lab_data/m51_mat_ruiyi/m51_3preirc_den_3\n",
      "using ind:  -1 final logll :  15.557111876351494\n",
      "tensor([[1.1422],\n",
      "        [0.8885],\n",
      "        [0.5519],\n",
      "        [0.1930]])\n"
     ]
    }
   ],
   "source": [
    "thetas={}\n",
    "for idensity in range(4):\n",
    "    datapath = Path(resdir/f'{m}_mat_ruiyi/preirc_den_{idensity}')\n",
    "    savename = datapath.parent/(f'{m}_{idensity}'+datapath.name)\n",
    "    invfile=savename\n",
    "    print(datapath.name)\n",
    "    finaltheta, finalcov, err = process_inv(\n",
    "        invfile, removegr=False, usingbest=False)\n",
    "    print(finaltheta[:4])\n",
    "    # finaltheta[0]=1\n",
    "    # finaltheta[1]=1.3\n",
    "    # finaltheta[1]=0.5\n",
    "    # finaltheta[1]=0.2\n",
    "    thetas[idensity]=finaltheta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute likelihood\n",
    "todo, need to do this trial by trial to take account of density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use computed likelihood from day 0615\n"
     ]
    }
   ],
   "source": [
    "# df likelihood\n",
    "today='0615'\n",
    "skipll=not(date.today().strftime(\"%m%d\") == today)\n",
    "\n",
    "def lltrial(state, action, task, finaltheta, samples=5):\n",
    "    with torch.no_grad():\n",
    "        return monkeyloss_(agent, action, np.array(task).reshape(1,-1), phi, finaltheta, env, action_var=0.01, num_iteration=1, states=state, samples=samples, gpu=False).item()\n",
    "\n",
    "likelihood_df=defaultdict(list)\n",
    "\n",
    "if not skipll: \n",
    "    # compute new ll and save with today date\n",
    "    for sess in session:\n",
    "        sessdf=df[df.session==sess]\n",
    "        state, action, task = df.state.to_list(), df.action.to_list(),df.task.to_list()\n",
    "        trial_idces=sessdf.trial\n",
    "        for trial_idx in trial_idces:\n",
    "            trialdf=sessdf[sessdf.trial==trial_idx]\n",
    "            trialdata=trialdf.iloc[0]\n",
    "            state, action, task = trialdata.state, trialdata.action, trialdata.task\n",
    "            trial_likelihood=lltrial([state], [action], [task], finaltheta) \n",
    "\n",
    "            likelihood_df['session'].append(sess)\n",
    "            likelihood_df['trial'].append(trial_idx)\n",
    "            likelihood_df['likelihood'].append(trial_likelihood)\n",
    "        \n",
    "    notify('all done')\n",
    "    today = date.today().strftime(\"%m%d\") # mm/dd\n",
    "    with open(resdir/f'{folder}/irc_ll_{today}','wb+') as f:\n",
    "        pickle.dump(likelihood_df,f)\n",
    "    notify('compute likelihood complete')\n",
    "    print(f'computed likelihood saved to {folder}/irc_ll_{today}')\n",
    "\n",
    "else: # use the given day\n",
    "    print(f'use computed likelihood from day {today}')\n",
    "    with open(resdir/f'{folder}/irc_ll_{today}','rb') as f:\n",
    "            likelihood_df=pickle.load(f)\n",
    "\n",
    "tmp=pd.DataFrame(likelihood_df)\n",
    "df = pd.merge(df, tmp, on=['trial','session'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0.005)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denslookup={0.0001:0, 0.0005:1, 0.001:2,  0.005:3}\n",
    "thisdensity=trialdf.density.item()\n",
    "denslookup[thisdensity],thisdensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use computed likelihood from day 0615\n"
     ]
    }
   ],
   "source": [
    "# df belief\n",
    "today='0615'\n",
    "skipblief=not(date.today().strftime(\"%m%d\") == today)\n",
    "\n",
    "def lltrial(state, action, task, finaltheta, samples=5):\n",
    "    with torch.no_grad():\n",
    "        return monkeyloss_(agent, action, np.array(task).reshape(1,-1), phi, finaltheta, env, action_var=0.01, num_iteration=1, states=state, samples=samples, gpu=False).item()\n",
    "\n",
    "belief_df=defaultdict(list)\n",
    "\n",
    "if not skipblief:\n",
    "    for sess in session:\n",
    "        sessdf=df[df.session==sess]\n",
    "        state, action, task = df.state.to_list(), df.action.to_list(),df.task.to_list()\n",
    "        trial_idces=sessdf.trial\n",
    "        for trial_idx in trial_idces:\n",
    "            trialdf=sessdf[sessdf.trial==trial_idx]\n",
    "            trialdata=trialdf.iloc[0]\n",
    "            state, action, task = trialdata.state, trialdata.action, trialdata.task\n",
    "            thisdensity=trialdf.density.item()\n",
    "            theta=thetas[denslookup[thisdensity]]\n",
    "\n",
    "            _, _, ep_belief, ep_rawcov = run_trials(agent=agent, \n",
    "                                                   env=env, phi=phi, theta=theta,          task=task, ntrials=1,\n",
    "                                                    pert=None, given_obs=None, return_belief=True, given_action=action, given_state=state)\n",
    "            # trial info\n",
    "            belief_df['session'].append(sess)\n",
    "            belief_df['trial'].append(trial_idx)\n",
    "\n",
    "            # belief\n",
    "            if len(state)<5: # \n",
    "                belief_df['belief'].append(np.nan)\n",
    "                belief_df['rawcov'].append(np.nan)\n",
    "            else:\n",
    "                init=torch.tensor(state[0]).reshape(-1,1)\n",
    "                trial_belief=(ep_belief[0]-ep_belief[0][0]+init)\n",
    "                belief_df['belief'].append(np.array(trial_belief)[:,:,0])\n",
    "                belief_df['rawcov'].append(np.array(ep_rawcov[0]))\n",
    "        \n",
    "    notify('all done')\n",
    "    today = date.today().strftime(\"%m%d\") # mm/dd\n",
    "    with open(resdir/f'{folder}/irc_belief_{today}','wb+') as f:\n",
    "        pickle.dump(belief_df,f)\n",
    "    notify('compute likelihood complete')\n",
    "    print(f'computed likelihood saved to {folder}/irc_belief_{today}')\n",
    "\n",
    "else: # use the given day\n",
    "    print(f'use computed likelihood from day {today}')\n",
    "    with open(resdir/f'{folder}/irc_belief_{today}','rb') as f:\n",
    "            belief_df=pickle.load(f)\n",
    "tmp=pd.DataFrame(belief_df)\n",
    "tmp=tmp.rename(columns={'cov': 'rawcov'})\n",
    "df = pd.merge(df, tmp, on=['trial','session'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['trial', 'session', 'fullon', 'density', 'eye_hori', 'eye_vert',\n",
       "        'ff_hori', 'ff_vert', 'fx', 'fy', 'mx', 'my', 'mv', 'mw', 'PPC', 'PFC',\n",
       "        'MST', 'error', 'error_sign', 'target_angle', 'target_distance',\n",
       "        'state', 'action', 'task', 'likelihood', 'belief', 'rawcov'],\n",
       "       dtype='object'),\n",
       " Index(['session', 'trial', 'belief', 'rawcov'], dtype='object'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns, tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack the belief state\n",
    "df['bmx']=df.apply(lambda x:x.belief[:,1]*worldscale, axis=1)\n",
    "df['bmy']=df.apply(lambda x:x.belief[:,0]*worldscale, axis=1)\n",
    "df['heading']=df.apply(lambda x: x.state[:,2]*180/pi, axis=1)\n",
    "df['belief_heading']=df.apply(lambda x: x.belief[:,2]*180/pi, axis=1)\n",
    "df['timer']=df.apply(lambda x: np.arange(len(x.mx)), axis=1)\n",
    "df['countdown']=df.apply(lambda x: np.flip(-np.arange(len(x.mx)), axis=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessdata=defaultdict(list)\n",
    "\n",
    "for sess in session:\n",
    "    sessdf=df[df.session==sess]\n",
    "    states, actions, tasks = sessdf.state.to_list(), sessdf.action.to_list(),sessdf.task.to_list()\n",
    "    beliefs,rawcovs=sessdf['belief'].to_list(),sessdf['rawcov'].to_list()\n",
    "\n",
    "    sess_latentff_hori, sess_latentff_vert = [], []\n",
    "    for ep_beliefs, ep_rawcovs, task in zip(beliefs, rawcovs, tasks): # process for each trial\n",
    "        mx, my, body_theta,  mv, mw = zip(*ep_beliefs)\n",
    "        body_theta = -(np.cumsum(mw) * 0.1-pi/2)\n",
    "        body_x, body_y = np.asarray(my).reshape(-1).astype('float') * \\\n",
    "            worldscale, np.asarray(mx).reshape(-1).astype('float')*worldscale\n",
    "\n",
    "        fx, fy = task[1]*worldscale, task[0]*worldscale\n",
    "        rel_dist = ((fx-body_x)**2+(fy-body_y)**2)**0.5\n",
    "        hor_theta_, ver_theta_ = convert_location_to_angle(abs(np.array(rel_dist)).reshape(-1).astype('float'), np.array(fx).reshape(-1).astype('float'), np.array(fy).reshape(-1).astype('float'),\n",
    "                                                           body_theta.astype('float'), body_x.astype(\n",
    "                                                               'float'), body_y.astype('float'),\n",
    "                                                           np.array(rel_dist).reshape(-1).astype('float'), # use the true eye positions to remove pre saccade movement and after overshooting eye movement\n",
    "                                                           np.array(rel_dist).reshape(-1).astype('float'), DT=0.1, remove_pre=False, remove_post=False)\n",
    "        # plt.plot(hor_theta_, ver_theta_, 'g')\n",
    "        sess_latentff_hori.append(hor_theta_)\n",
    "        sess_latentff_vert.append(ver_theta_)\n",
    "    sessdata['belief_ff_hori']+=[a.reshape(-1) for a in sess_latentff_hori]\n",
    "    sessdata['belief_ff_vert']+=[a.reshape(-1) for a in sess_latentff_vert]\n",
    "    \n",
    "\n",
    "# TODO temp need change to merge\n",
    "df['belief_ff_hori']=sessdata['belief_ff_hori']\n",
    "df['belief_ff_vert']=sessdata['belief_ff_vert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angle from start\n",
    "def get_angle_from_start(row):\n",
    "    return np.arctan2((np.array(row.my)), (np.array(row.mx)))\n",
    "\n",
    "df['angle_from_start']=df.apply(get_angle_from_start, axis=1)\n",
    "\n",
    "def get_belief_angle_from_start(row):\n",
    "    return np.arctan2((np.array(row.bmy)), (np.array(row.bmx)))\n",
    "\n",
    "df['belief_angle_from_start']=df.apply(get_angle_from_start, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: eye_hori, Lengths: [25, 22, 15]\n",
      "Column: eye_vert, Lengths: [25, 22, 15]\n",
      "Column: ff_hori, Lengths: [25, 22, 15]\n",
      "Column: ff_vert, Lengths: [25, 22, 15]\n",
      "Column: fx, Lengths: [25, 22, 15]\n",
      "Column: fy, Lengths: [25, 22, 15]\n",
      "Column: mx, Lengths: [25, 22, 15]\n",
      "Column: my, Lengths: [25, 22, 15]\n",
      "Column: mv, Lengths: [25, 22, 15]\n",
      "Column: mw, Lengths: [25, 22, 15]\n",
      "Column: PPC, Lengths: [25, 22, 15]\n",
      "Column: state, Lengths: [25, 22, 15]\n",
      "Column: action, Lengths: [25, 22, 15]\n",
      "Column: task, Lengths: [2, 2, 2]\n",
      "Column: belief, Lengths: [25, 22, 15]\n",
      "Column: rawcov, Lengths: [25, 22, 15]\n",
      "Column: bmx, Lengths: [25, 22, 15]\n",
      "Column: bmy, Lengths: [25, 22, 15]\n",
      "Column: heading, Lengths: [25, 22, 15]\n",
      "Column: belief_heading, Lengths: [25, 22, 15]\n",
      "Column: timer, Lengths: [25, 22, 15]\n",
      "Column: countdown, Lengths: [25, 22, 15]\n",
      "Column: belief_ff_hori, Lengths: [25, 22, 15]\n",
      "Column: belief_ff_vert, Lengths: [25, 22, 15]\n",
      "Column: angle_from_start, Lengths: [25, 22, 15]\n",
      "Column: belief_angle_from_start, Lengths: [25, 22, 15]\n"
     ]
    }
   ],
   "source": [
    "# check len\n",
    "random_rows = df.sample(n=3)\n",
    "\n",
    "# Iterate over the columns\n",
    "for col in random_rows.columns:\n",
    "    try:\n",
    "        # Get the column values for the selected rows\n",
    "        col_values = random_rows[col]\n",
    "        # Calculate the length of each column value\n",
    "        lengths = col_values.apply(lambda x: len(x))\n",
    "        # Print column name and the length of each column value\n",
    "        print(f\"Column: {col}, Lengths: {lengths.tolist()}\")\n",
    "    except: continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: ff_hori, Lengths: [(16,), (22,), (10,)]\n",
      "Column: ff_vert, Lengths: [(16,), (22,), (10,)]\n",
      "Column: PPC, Lengths: [(16, 94), (22, 112), (10, 94)]\n",
      "Column: state, Lengths: [(16, 5), (22, 5), (10, 5)]\n",
      "Column: action, Lengths: [(16, 2), (22, 2), (10, 2)]\n",
      "Column: belief, Lengths: [(16, 5), (22, 5), (10, 5)]\n",
      "Column: rawcov, Lengths: [(16, 5, 5), (22, 5, 5), (10, 5, 5)]\n",
      "Column: bmx, Lengths: [(16,), (22,), (10,)]\n",
      "Column: bmy, Lengths: [(16,), (22,), (10,)]\n",
      "Column: heading, Lengths: [(16,), (22,), (10,)]\n",
      "Column: belief_heading, Lengths: [(16,), (22,), (10,)]\n",
      "Column: timer, Lengths: [(16,), (22,), (10,)]\n",
      "Column: countdown, Lengths: [(16,), (22,), (10,)]\n",
      "Column: belief_ff_hori, Lengths: [(16,), (22,), (10,)]\n",
      "Column: belief_ff_vert, Lengths: [(16,), (22,), (10,)]\n",
      "Column: angle_from_start, Lengths: [(16,), (22,), (10,)]\n",
      "Column: belief_angle_from_start, Lengths: [(16,), (22,), (10,)]\n"
     ]
    }
   ],
   "source": [
    "# check size\n",
    "random_rows = df.sample(n=3)\n",
    "\n",
    "# Iterate over the columns\n",
    "for col in random_rows.columns:\n",
    "    try:\n",
    "        # Get the column values for the selected rows\n",
    "        col_values = random_rows[col]\n",
    "        # Calculate the length of each column value\n",
    "        lengths = col_values.apply(lambda x: (x.shape))\n",
    "        # Print column name and the length of each column value\n",
    "        print(f\"Column: {col}, Lengths: {lengths.tolist()}\")\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the varialbes we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, change coord example\n",
    "trialdf=df.iloc[10]\n",
    "mx, my,mw, fx,fy=trialdf.mx, trialdf.my, trialdf.mw, trialdf.fx, trialdf.fy\n",
    "mx, my,mw, fx,fy=[np.array(a) for a in [mx, my,mw, fx,fy]]\n",
    "sx = np.ones_like(fx)\n",
    "sy = np.ones_like(fy)\n",
    "if my.size > 0:\n",
    "    fx = np.ones_like(fx) * fx[0]\n",
    "    fy = np.ones_like(fy) * fy[0]\n",
    "    sx *= mx[-1]\n",
    "    sy *= my[-1]\n",
    "    my = my + 30\n",
    "    fy = fy + 30\n",
    "    sy = sy + 30\n",
    "dx = fx - mx; dy = fy - my\n",
    "rel_dist = np.sqrt(dx**2 + dy**2); rel_ang = np.rad2deg(np.arctan2(dy, dx))\n",
    "# rel_dist_stop = np.sqrt((sx - mx)**2 + (sy - my)**2)\n",
    "# abs_dist = np.sqrt(mx**2 + my**2); abs_ang = np.rad2deg(np.arctan2(my, mx))\n",
    "heading = np.deg2rad(np.cumsum(mw*-1) * 0.1 + 90) \n",
    "\n",
    "latent_ff_hori, latent_ff_vert = convert_location_to_angle(\n",
    "    rel_dist,\n",
    "    fx,\n",
    "    fy,\n",
    "    heading,\n",
    "    mx,\n",
    "    my,\n",
    "    None, # not needed if remove pre = false\n",
    "    None,\n",
    "    remove_pre=False\n",
    ")\n",
    "# plt.scatter(latent_ff_hori,trialdf.ff_hori); plt.title('state');plt.axis('equal'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # belief chnage coord example\n",
    "trialdf=df.iloc[23]\n",
    "\n",
    "bmx, bmy, bmw, fx,fy = trialdf.bmx, trialdf.bmy, np.array(trialdf.mw), trialdf.fx, trialdf.fy\n",
    "ep_beliefs=trialdf.belief\n",
    "\n",
    "mx, my, body_theta,  mv, mw = zip(*ep_beliefs)\n",
    "mx, my,mw, fx,fy=[np.array(a) for a in [mx, my,mw, fx,fy]]\n",
    "mx,my=my*worldscale, mx*worldscale\n",
    "\n",
    "# plt.scatter(mx, bmx); plt.title('beliefmx vs bmx');plt.axis('equal'); plt.show()\n",
    "\n",
    "sx = np.ones_like(fx)\n",
    "sy = np.ones_like(fy)\n",
    "if my.size > 0:\n",
    "    fx = np.ones_like(fx) * fx[0]\n",
    "    fy = np.ones_like(fy) * fy[0]\n",
    "    sx *= mx[-1]\n",
    "    sy *= my[-1]\n",
    "    my = my + 30\n",
    "    fy = fy + 30\n",
    "    sy = sy + 30\n",
    "dx = fx - mx; dy = fy - my\n",
    "rel_dist = np.sqrt(dx**2 + dy**2); rel_ang = np.rad2deg(np.arctan2(dy, dx))\n",
    "# rel_dist_stop = np.sqrt((sx - mx)**2 + (sy - my)**2)\n",
    "# abs_dist = np.sqrt(mx**2 + my**2); abs_ang = np.rad2deg(np.arctan2(my, mx))\n",
    "\n",
    "heading2=-(np.cumsum(mw) * 0.1-pi/2)\n",
    "\n",
    "latent_ff_hori, latent_ff_vert = convert_location_to_angle(\n",
    "    rel_dist,\n",
    "    fx,\n",
    "    fy,\n",
    "    heading2,\n",
    "    mx,\n",
    "    my,\n",
    "    None, # not needed if remove pre = false\n",
    "    None,\n",
    "    remove_pre=False\n",
    ")\n",
    "# plt.scatter(latent_ff_hori,trialdf.belief_ff_hori); plt.title('belief');plt.axis('equal'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([180.00067, 180.00067, 179.62195, 179.36865, 179.58482, 181.6971 ,\n",
       "       186.22366, 191.78711, 196.95834, 201.84085, 204.80537, 205.76024,\n",
       "       206.60269, 207.36252, 208.06198, 209.04297, 209.75887, 211.42252],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert belief rawcov example\n",
    "trialdf=df.iloc[23]\n",
    "\n",
    "bmx, bmy, bmw, fx,fy = trialdf.bmx, trialdf.bmy, np.array(trialdf.mw), trialdf.fx, trialdf.fy\n",
    "rawcov=trialdf['rawcov'][:,:2,:2]\n",
    "\n",
    "\n",
    "belief_heading=trialdf.belief_heading\n",
    "rotdegree=belief_heading+180\n",
    "relativeposrawcov=[]\n",
    "for degree, thisrawcov in zip(rotdegree, rawcov):\n",
    "    R=np.array([[np.cos(-degree/180*pi),-np.sin(-degree/180*pi)],[np.sin(-degree/180*pi),np.cos(-degree/180*pi)]])\n",
    "    relativeposrawcov.append(R.T@thisrawcov[:2,:2]@R)\n",
    "relativeposrawcov=np.stack(relativeposrawcov)\n",
    "relativeposrawcov.shape\n",
    "rotdegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate belief cov df\n",
    "def fun(trialdf):\n",
    "    if trialdf.fullon==1: # always on target, assign zero uncertainty\n",
    "        return np.ones_like(trialdf['rawcov'][:,:2,:2])*1e-6\n",
    "    cov=trialdf['rawcov'][:,:2,:2]\n",
    "    belief_heading=trialdf.belief_heading\n",
    "    rotdegree=belief_heading+180\n",
    "    relativeposcov=[]\n",
    "    for degree, thiscov in zip(rotdegree, cov):\n",
    "        R=np.array([[np.cos(-degree/180*pi),-np.sin(-degree/180*pi)],[np.sin(-degree/180*pi),np.cos(-degree/180*pi)]])\n",
    "        relativeposcov.append(R.T@thiscov[:2,:2]@R)\n",
    "    relativeposcov=np.stack(relativeposcov)*worldscale*worldscale\n",
    "    return relativeposcov\n",
    "df['relcov']=df.apply(fun, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: 0918_m51df.pkl\n"
     ]
    }
   ],
   "source": [
    "date='0918'\n",
    "df.to_pickle(resdir/f'{date}_m51df.pkl')\n",
    "notify(f'saved: {date}_m51df.pkl')\n",
    "print(f'saved: {date}_m51df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
